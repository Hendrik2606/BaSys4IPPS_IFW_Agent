{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import scipy.stats\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import entropy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import lag_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#import pywt\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input: str = \"./train\" # input path\n",
    "sensor: int = 5 # input sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 files!\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(path_input, str) or not (path:=Path(path_input)).is_dir():\n",
    "    raise ValueError(f\"Input {path_input} is not a valid directory!\")\n",
    "\n",
    "all_files = glob.glob(path.as_posix() + \"/*.csv\")\n",
    "\n",
    "if len(all_files) == 0:\n",
    "    raise Exception(\"No files. Abort.\")\n",
    "\n",
    "print(f\"Found {len(all_files)} files!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerri\\AppData\\Local\\Temp\\ipykernel_9572\\1875244818.py:14: DtypeWarning: Columns (7,9,13,23,26,32,37,47,50,55,83,87,91,105,108,117,127,129,138,149,151,155,165,168,174,179,189,192,197,225,229,233,247,250,259,269,271,280,291,293,297,307,310,316,321,331,334,339,367,371,375,389,392,401,411,413,422,433,435,439,449,452,458,463,473,476,481,509,513,517,531,534,543,553,555,564,575,577,581,591,594,600,605,615,618,623,651,655,659,673,676,685,695,697,706,717,719,723,733,736,742,747,757,760,765,793,797,801,815,818,827,837,839,848,859,861,865,875,878,884,889,899,902,907,935,939,943,957,960,969,979,981,990,1001,1003,1007,1017,1020,1026,1031,1041,1044,1049,1077,1081,1085,1099,1102,1111,1121,1123,1132,1143,1145,1149,1159,1162,1168,1173,1183,1186,1191,1219,1223,1227,1241,1244,1253,1263,1265,1274,1285,1287,1291,1301,1304,1310,1315,1325,1328,1333,1361,1365,1369,1383,1386,1395,1405,1407,1416,1427,1429,1433,1443,1446,1452,1457,1467,1470,1475,1503,1507,1511,1525,1528,1537,1547,1549,1558,1569,1571,1575,1585,1588,1594,1599,1609,1612,1617,1645,1649,1653,1667,1670,1679,1689,1691,1700,1711,1713,1717,1727,1730,1736,1741,1751,1754,1759,1787,1791,1795,1809,1812,1821,1831,1833,1842,1853,1855,1859,1869,1872,1878,1883,1893,1896,1901,1929,1933,1937,1951,1954,1963,1973,1975,1984,1995,1997,2001,2011,2014,2020,2025,2035,2038,2043,2071,2075,2079,2093,2096,2105,2115,2117,2126,2137,2139,2143,2153,2156,2162,2167,2177,2180,2185,2213,2217,2221,2235,2238,2247,2257,2259,2268) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(all_files[sensor - 1], decimal=\",\", sep=\";\", header=0)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# Segmentieren der Zeitreihen\n",
    "\n",
    "# Loesche ab Begin_Del 1769 / 710\n",
    "Begin_Del = 710\n",
    "# Loesche die ersten Teil\n",
    "To_Del = 20\n",
    "\n",
    "Num_Pro = 139\n",
    "\n",
    "\n",
    "# Laden der Trainingsdaten\n",
    "train_data = pd.read_csv(all_files[sensor - 1], decimal=\",\", sep=\";\", header=0)\n",
    "\n",
    "# Segmentieren der Daten\n",
    "\n",
    "train_data = train_data.drop(train_data.index[Begin_Del : len(train_data)])\n",
    "train_data = train_data.drop(train_data.index[0:To_Del])\n",
    "\n",
    "# Datenformat float\n",
    "train_data = train_data.to_numpy().astype(\"float64\")\n",
    "\n",
    "# Variable for time entries\n",
    "time = train_data[:, 0]\n",
    "\n",
    "# delete nan columns\n",
    "train_data = train_data[:, ~np.isnan(train_data).any(axis=0)]\n",
    "\n",
    "# delete first column (time entries)\n",
    "train_data = train_data[:, 1 : Num_Pro + 1]\n",
    "\n",
    "# Convert\n",
    "train_data[train_data == -2.8026e-45] = -0.0001\n",
    "train_data[train_data == 2.8026e-45] = 0.0001\n",
    "\n",
    "train_data[train_data == -2.8e-41] = -0.0001\n",
    "train_data[train_data == 2.8e-41] = 0.0001\n",
    "\n",
    "## Define X_train and X_test\n",
    "\n",
    "X_train = train_data[:, 0:20]\n",
    "X_test = train_data[:, 115]\n",
    "X_test = X_test.transpose().reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create configuration for training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basys4ipps_ifw_agent.Agent.basys_agent import BasysAgent, BasysConfig\n",
    "\n",
    "basys_config = BasysConfig()\n",
    "basys_config.alpha_safety_factor = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "{'contamination': 0.1, 'n_neighbors': 5, 'method': 'largest', 'radius': 1.0, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'p': 2, 'metric_params': None, 'n_jobs': 1, 'neigh_': NearestNeighbors(n_jobs=1), '_classes': 2, 'tree_': <sklearn.neighbors._kd_tree.KDTree object at 0x00000196633AFD30>, 'decision_scores_': array([7.22462258, 2.92445374, 4.04769915, 3.62095359, 4.59976424,\n",
      "       3.97825938, 3.62095359, 2.65778377, 1.77502885, 1.91802469,\n",
      "       1.47655424, 0.        , 1.91671737, 1.84873355, 0.        ,\n",
      "       0.        , 0.        , 1.6667368 , 0.        , 0.        ]), 'threshold_': 4.1029056576726415, 'labels_': array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), '_mu': 2.163814277115093, '_sigma': 1.90577831155715}\n",
      "Start testing\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m agent\u001b[39m.\u001b[39mfit(X_train)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart testing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m score_prob, alarm, exp_downtime \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Ende Daten laden ######################################\n",
    "\n",
    "# initialise model\n",
    "# baseline_model = unified_outlier_score(base_model = \"KNN\", outlier_score_scaling = \"gaussian\", alpha_safety_factor = 0.99999, feature_group = \"general_purpose\", feature_scaling_method = \"standardize\", learning_type = \"static\", machine_component = \"axis_drive\", segementation_start = 20, segmentation_end = 710)\n",
    "\n",
    "# fit the model on train data\n",
    "# baseline_model.fit(X_train)\n",
    "\n",
    "# predict using baseline model\n",
    "# score_prob, alarm, exp_downtime = baseline_model.predict(X_test)\n",
    "\n",
    "\n",
    "agent = BasysAgent(basys_config)\n",
    "\n",
    "print(\"Start training\")\n",
    "agent.fit(X_train)\n",
    "\n",
    "print(\"Start testing\")\n",
    "score_prob, alarm, exp_downtime = agent.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
